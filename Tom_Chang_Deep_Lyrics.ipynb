{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tom-Chang-Deep-Lyrics",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eric999j/PILTest/blob/master/Tom_Chang_Deep_Lyrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky38jmSUorCR",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/fukuball/Tom-Chang-Deep-Lyrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoG2FkDbmda4",
        "colab_type": "code",
        "outputId": "3b79eb5e-6461-4ff0-ea86-64084feda320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsivFx9BnJ7k",
        "colab_type": "code",
        "outputId": "13a4abb2-9b26-492b-8572-e655835f32cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%cd /content/gdrive/'My Drive'/'Colab Notebooks'/Tom-Chang-Deep-Lyrics-master/\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Tom-Chang-Deep-Lyrics-master\n",
            "Config.py    \u001b[0m\u001b[01;34mlyrics\u001b[0m/   Model.pyc     requirements_test.txt\n",
            "generate.py  \u001b[01;34mmodel\u001b[0m/    \u001b[01;34m__pycache__\u001b[0m/  requirements.txt\n",
            "LICENSE      Model.py  README.md     train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXBT0T-6n6Cl",
        "colab_type": "code",
        "outputId": "e5e94f1c-ad75-4f32-fe22-da192cd7c09b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "%pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1MB 82kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (1.17.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (0.33.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (0.2.2)\n",
            "Collecting tensorboard<1.9.0,>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0->-r requirements.txt (line 1)) (3.1.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0->-r requirements.txt (line 1)) (42.0.2)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107221 sha256=f5109c4bf503cea92fe8aa40063aec607abeb2cdb23e31320ecf521ef633e584\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.8.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZIgAgBXobsa",
        "colab_type": "code",
        "outputId": "ac5f8662-a93c-4274-be22-41b50fa3dd72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%mkdir model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITdhPZC_BbHV",
        "colab_type": "code",
        "outputId": "b63f2f85-f0cc-4aae-fb35-25fb24e15d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python -V"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz7PiS0vGELp",
        "colab_type": "code",
        "outputId": "0cc0acbb-8cce-4994-f1c3-5e0629a3bacb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python train.py lyrics/all_training_lyrics.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "data has 74856 characters, 2612 unique.\n",
            "2019-12-27 02:19:24.897421: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4c80d05908>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
            "Training Epoch: 1 ...\n",
            "0.10 perplexity: 1393.107 cost-time: 74.87 s\n",
            "0.19 perplexity: 820.303 cost-time: 66.55 s\n",
            "0.29 perplexity: 662.153 cost-time: 66.45 s\n",
            "0.39 perplexity: 593.262 cost-time: 66.60 s\n",
            "0.48 perplexity: 546.730 cost-time: 66.48 s\n",
            "0.58 perplexity: 523.698 cost-time: 67.03 s\n",
            "0.68 perplexity: 506.876 cost-time: 70.40 s\n",
            "0.77 perplexity: 494.792 cost-time: 72.29 s\n",
            "0.87 perplexity: 481.390 cost-time: 72.41 s\n",
            "0.97 perplexity: 469.363 cost-time: 72.34 s\n",
            "Epoch: 1 Train Perplexity: 468.711\n",
            "Training Epoch: 2 ...\n",
            "0.10 perplexity: 1380.988 cost-time: 79.69 s\n",
            "0.19 perplexity: 735.629 cost-time: 71.98 s\n",
            "0.29 perplexity: 581.686 cost-time: 72.15 s\n",
            "0.39 perplexity: 519.140 cost-time: 71.90 s\n",
            "0.48 perplexity: 479.276 cost-time: 71.90 s\n",
            "0.58 perplexity: 459.257 cost-time: 71.87 s\n",
            "0.68 perplexity: 445.858 cost-time: 71.52 s\n",
            "0.77 perplexity: 436.751 cost-time: 71.20 s\n",
            "0.87 perplexity: 426.407 cost-time: 72.00 s\n",
            "0.97 perplexity: 416.545 cost-time: 71.82 s\n",
            "Epoch: 2 Train Perplexity: 415.975\n",
            "Training Epoch: 3 ...\n",
            "0.10 perplexity: 353.770 cost-time: 80.13 s\n",
            "0.19 perplexity: 345.976 cost-time: 72.14 s\n",
            "0.29 perplexity: 338.823 cost-time: 72.19 s\n",
            "0.39 perplexity: 336.351 cost-time: 71.88 s\n",
            "0.48 perplexity: 330.135 cost-time: 71.57 s\n",
            "0.58 perplexity: 328.618 cost-time: 71.98 s\n",
            "0.68 perplexity: 328.417 cost-time: 71.85 s\n",
            "0.77 perplexity: 327.647 cost-time: 71.77 s\n",
            "0.87 perplexity: 324.030 cost-time: 71.52 s\n",
            "0.97 perplexity: 319.611 cost-time: 67.81 s\n",
            "Epoch: 3 Train Perplexity: 319.876\n",
            "Training Epoch: 4 ...\n",
            "0.10 perplexity: 294.874 cost-time: 73.60 s\n",
            "0.19 perplexity: 289.987 cost-time: 66.16 s\n",
            "0.29 perplexity: 283.430 cost-time: 66.32 s\n",
            "0.39 perplexity: 281.572 cost-time: 66.31 s\n",
            "0.48 perplexity: 277.253 cost-time: 66.30 s\n",
            "0.58 perplexity: 276.753 cost-time: 66.29 s\n",
            "0.68 perplexity: 276.670 cost-time: 66.30 s\n",
            "0.77 perplexity: 275.712 cost-time: 66.38 s\n",
            "0.87 perplexity: 272.667 cost-time: 67.25 s\n",
            "0.97 perplexity: 269.236 cost-time: 71.01 s\n",
            "Epoch: 4 Train Perplexity: 269.365\n",
            "Training Epoch: 5 ...\n",
            "0.10 perplexity: 244.892 cost-time: 79.36 s\n",
            "0.19 perplexity: 242.197 cost-time: 71.67 s\n",
            "0.29 perplexity: 237.248 cost-time: 71.33 s\n",
            "0.39 perplexity: 236.491 cost-time: 71.66 s\n",
            "0.48 perplexity: 232.826 cost-time: 71.53 s\n",
            "0.58 perplexity: 233.224 cost-time: 71.43 s\n",
            "0.68 perplexity: 233.710 cost-time: 71.65 s\n",
            "0.77 perplexity: 233.214 cost-time: 71.61 s\n",
            "0.87 perplexity: 231.124 cost-time: 71.59 s\n",
            "0.97 perplexity: 228.605 cost-time: 71.04 s\n",
            "Epoch: 5 Train Perplexity: 228.636\n",
            "model saving ...\n",
            "Done!\n",
            "Training Epoch: 6 ...\n",
            "0.10 perplexity: 208.804 cost-time: 80.71 s\n",
            "0.19 perplexity: 209.989 cost-time: 72.21 s\n",
            "0.29 perplexity: 205.468 cost-time: 71.79 s\n",
            "0.39 perplexity: 204.662 cost-time: 72.31 s\n",
            "0.48 perplexity: 201.346 cost-time: 72.07 s\n",
            "0.58 perplexity: 201.016 cost-time: 71.98 s\n",
            "0.68 perplexity: 201.153 cost-time: 72.12 s\n",
            "0.77 perplexity: 200.657 cost-time: 72.08 s\n",
            "0.87 perplexity: 198.930 cost-time: 72.21 s\n",
            "0.97 perplexity: 197.096 cost-time: 72.09 s\n",
            "Epoch: 6 Train Perplexity: 197.027\n",
            "Training Epoch: 7 ...\n",
            "0.10 perplexity: 178.003 cost-time: 80.21 s\n",
            "0.19 perplexity: 178.818 cost-time: 69.84 s\n",
            "0.29 perplexity: 174.932 cost-time: 66.37 s\n",
            "0.39 perplexity: 174.067 cost-time: 66.40 s\n",
            "0.48 perplexity: 171.765 cost-time: 66.42 s\n",
            "0.58 perplexity: 171.634 cost-time: 66.43 s\n",
            "0.68 perplexity: 171.450 cost-time: 66.46 s\n",
            "0.77 perplexity: 171.349 cost-time: 66.64 s\n",
            "0.87 perplexity: 170.044 cost-time: 66.70 s\n",
            "0.97 perplexity: 168.679 cost-time: 66.59 s\n",
            "Epoch: 7 Train Perplexity: 168.376\n",
            "Training Epoch: 8 ...\n",
            "0.10 perplexity: 149.158 cost-time: 73.93 s\n",
            "0.19 perplexity: 151.612 cost-time: 66.48 s\n",
            "0.29 perplexity: 149.868 cost-time: 66.59 s\n",
            "0.39 perplexity: 149.590 cost-time: 66.49 s\n",
            "0.48 perplexity: 147.057 cost-time: 66.53 s\n",
            "0.58 perplexity: 146.769 cost-time: 66.73 s\n",
            "0.68 perplexity: 146.858 cost-time: 66.51 s\n",
            "0.77 perplexity: 146.503 cost-time: 66.82 s\n",
            "0.87 perplexity: 145.493 cost-time: 66.65 s\n",
            "0.97 perplexity: 144.280 cost-time: 66.68 s\n",
            "Epoch: 8 Train Perplexity: 144.094\n",
            "Training Epoch: 9 ...\n",
            "0.10 perplexity: 126.339 cost-time: 74.01 s\n",
            "0.19 perplexity: 128.807 cost-time: 66.66 s\n",
            "0.29 perplexity: 127.550 cost-time: 66.74 s\n",
            "0.39 perplexity: 127.117 cost-time: 66.61 s\n",
            "0.48 perplexity: 124.922 cost-time: 66.67 s\n",
            "0.58 perplexity: 125.174 cost-time: 66.75 s\n",
            "0.68 perplexity: 125.767 cost-time: 66.85 s\n",
            "0.77 perplexity: 125.126 cost-time: 66.64 s\n",
            "0.87 perplexity: 124.156 cost-time: 66.60 s\n",
            "0.97 perplexity: 123.381 cost-time: 66.85 s\n",
            "Epoch: 9 Train Perplexity: 123.320\n",
            "Training Epoch: 10 ...\n",
            "0.10 perplexity: 109.553 cost-time: 74.39 s\n",
            "0.19 perplexity: 111.629 cost-time: 67.02 s\n",
            "0.29 perplexity: 110.292 cost-time: 66.70 s\n",
            "0.39 perplexity: 109.630 cost-time: 66.56 s\n",
            "0.48 perplexity: 107.567 cost-time: 66.41 s\n",
            "0.58 perplexity: 107.067 cost-time: 66.65 s\n",
            "0.68 perplexity: 107.530 cost-time: 66.73 s\n",
            "0.77 perplexity: 107.098 cost-time: 66.61 s\n",
            "0.87 perplexity: 106.259 cost-time: 66.61 s\n",
            "0.97 perplexity: 105.851 cost-time: 66.78 s\n",
            "Epoch: 10 Train Perplexity: 105.686\n",
            "model saving ...\n",
            "Done!\n",
            "Training Epoch: 11 ...\n",
            "0.10 perplexity: 94.603 cost-time: 74.41 s\n",
            "0.19 perplexity: 96.275 cost-time: 66.62 s\n",
            "0.29 perplexity: 94.658 cost-time: 66.62 s\n",
            "0.39 perplexity: 93.980 cost-time: 66.78 s\n",
            "0.48 perplexity: 91.989 cost-time: 67.12 s\n",
            "0.58 perplexity: 91.889 cost-time: 66.78 s\n",
            "0.68 perplexity: 92.234 cost-time: 66.71 s\n",
            "0.77 perplexity: 91.902 cost-time: 66.87 s\n",
            "0.87 perplexity: 91.538 cost-time: 66.92 s\n",
            "0.97 perplexity: 91.384 cost-time: 66.87 s\n",
            "Epoch: 11 Train Perplexity: 91.247\n",
            "Training Epoch: 12 ...\n",
            "0.10 perplexity: 79.706 cost-time: 74.18 s\n",
            "0.19 perplexity: 82.460 cost-time: 66.59 s\n",
            "0.29 perplexity: 80.623 cost-time: 66.57 s\n",
            "0.39 perplexity: 79.959 cost-time: 66.86 s\n",
            "0.48 perplexity: 78.557 cost-time: 66.69 s\n",
            "0.58 perplexity: 78.232 cost-time: 66.66 s\n",
            "0.68 perplexity: 78.441 cost-time: 66.97 s\n",
            "0.77 perplexity: 78.215 cost-time: 66.91 s\n",
            "0.87 perplexity: 78.019 cost-time: 66.66 s\n",
            "0.97 perplexity: 77.831 cost-time: 66.55 s\n",
            "Epoch: 12 Train Perplexity: 77.691\n",
            "Training Epoch: 13 ...\n",
            "0.10 perplexity: 69.605 cost-time: 74.18 s\n",
            "0.19 perplexity: 71.372 cost-time: 66.64 s\n",
            "0.29 perplexity: 69.524 cost-time: 66.79 s\n",
            "0.39 perplexity: 68.265 cost-time: 66.69 s\n",
            "0.48 perplexity: 66.889 cost-time: 67.08 s\n",
            "0.58 perplexity: 66.411 cost-time: 67.03 s\n",
            "0.68 perplexity: 66.615 cost-time: 67.25 s\n",
            "0.77 perplexity: 66.231 cost-time: 66.82 s\n",
            "0.87 perplexity: 66.084 cost-time: 66.65 s\n",
            "0.97 perplexity: 66.124 cost-time: 66.72 s\n",
            "Epoch: 13 Train Perplexity: 65.962\n",
            "Training Epoch: 14 ...\n",
            "0.10 perplexity: 57.622 cost-time: 74.22 s\n",
            "0.19 perplexity: 60.417 cost-time: 66.91 s\n",
            "0.29 perplexity: 59.383 cost-time: 66.78 s\n",
            "0.39 perplexity: 58.487 cost-time: 66.77 s\n",
            "0.48 perplexity: 57.791 cost-time: 66.75 s\n",
            "0.58 perplexity: 57.432 cost-time: 66.83 s\n",
            "0.68 perplexity: 57.899 cost-time: 66.60 s\n",
            "0.77 perplexity: 57.576 cost-time: 66.54 s\n",
            "0.87 perplexity: 57.266 cost-time: 66.38 s\n",
            "0.97 perplexity: 57.235 cost-time: 66.26 s\n",
            "Epoch: 14 Train Perplexity: 57.082\n",
            "Training Epoch: 15 ...\n",
            "0.10 perplexity: 49.526 cost-time: 73.87 s\n",
            "0.19 perplexity: 51.709 cost-time: 66.34 s\n",
            "0.29 perplexity: 51.118 cost-time: 66.42 s\n",
            "0.39 perplexity: 50.359 cost-time: 66.35 s\n",
            "0.48 perplexity: 49.797 cost-time: 66.52 s\n",
            "0.58 perplexity: 49.566 cost-time: 66.48 s\n",
            "0.68 perplexity: 49.922 cost-time: 66.33 s\n",
            "0.77 perplexity: 49.769 cost-time: 66.49 s\n",
            "0.87 perplexity: 49.565 cost-time: 66.41 s\n",
            "0.97 perplexity: 49.626 cost-time: 66.58 s\n",
            "Epoch: 15 Train Perplexity: 49.467\n",
            "model saving ...\n",
            "Done!\n",
            "Training Epoch: 16 ...\n",
            "0.10 perplexity: 43.625 cost-time: 74.13 s\n",
            "0.19 perplexity: 45.190 cost-time: 66.28 s\n",
            "0.29 perplexity: 44.754 cost-time: 66.43 s\n",
            "0.39 perplexity: 43.981 cost-time: 66.48 s\n",
            "0.48 perplexity: 43.354 cost-time: 66.37 s\n",
            "0.58 perplexity: 43.194 cost-time: 66.33 s\n",
            "0.68 perplexity: 43.467 cost-time: 66.38 s\n",
            "0.77 perplexity: 43.360 cost-time: 66.54 s\n",
            "0.87 perplexity: 43.233 cost-time: 66.71 s\n",
            "0.97 perplexity: 43.422 cost-time: 66.65 s\n",
            "Epoch: 16 Train Perplexity: 43.316\n",
            "Training Epoch: 17 ...\n",
            "0.10 perplexity: 37.969 cost-time: 73.86 s\n",
            "0.19 perplexity: 39.904 cost-time: 66.48 s\n",
            "0.29 perplexity: 39.381 cost-time: 66.68 s\n",
            "0.39 perplexity: 38.442 cost-time: 66.60 s\n",
            "0.48 perplexity: 37.876 cost-time: 66.51 s\n",
            "0.58 perplexity: 37.694 cost-time: 66.44 s\n",
            "0.68 perplexity: 37.592 cost-time: 66.40 s\n",
            "0.77 perplexity: 37.567 cost-time: 66.46 s\n",
            "0.87 perplexity: 37.389 cost-time: 66.53 s\n",
            "0.97 perplexity: 37.565 cost-time: 66.36 s\n",
            "Epoch: 17 Train Perplexity: 37.498\n",
            "Training Epoch: 18 ...\n",
            "0.10 perplexity: 34.616 cost-time: 73.70 s\n",
            "0.19 perplexity: 35.991 cost-time: 66.61 s\n",
            "0.29 perplexity: 35.405 cost-time: 66.43 s\n",
            "0.39 perplexity: 34.241 cost-time: 66.39 s\n",
            "0.48 perplexity: 33.548 cost-time: 66.54 s\n",
            "0.58 perplexity: 33.284 cost-time: 66.79 s\n",
            "0.68 perplexity: 33.195 cost-time: 66.90 s\n",
            "0.77 perplexity: 32.948 cost-time: 66.64 s\n",
            "0.87 perplexity: 32.811 cost-time: 66.46 s\n",
            "0.97 perplexity: 32.953 cost-time: 66.25 s\n",
            "Epoch: 18 Train Perplexity: 32.882\n",
            "Training Epoch: 19 ...\n",
            "0.10 perplexity: 30.584 cost-time: 73.80 s\n",
            "0.19 perplexity: 31.828 cost-time: 66.52 s\n",
            "0.29 perplexity: 31.336 cost-time: 66.31 s\n",
            "0.39 perplexity: 29.959 cost-time: 66.19 s\n",
            "0.48 perplexity: 29.201 cost-time: 66.38 s\n",
            "0.58 perplexity: 29.105 cost-time: 66.44 s\n",
            "0.68 perplexity: 28.981 cost-time: 66.27 s\n",
            "0.77 perplexity: 28.780 cost-time: 66.16 s\n",
            "0.87 perplexity: 28.732 cost-time: 66.29 s\n",
            "0.97 perplexity: 28.856 cost-time: 66.53 s\n",
            "Epoch: 19 Train Perplexity: 28.817\n",
            "Training Epoch: 20 ...\n",
            "0.10 perplexity: 28.858 cost-time: 73.63 s\n",
            "0.19 perplexity: 29.358 cost-time: 66.30 s\n",
            "0.29 perplexity: 29.067 cost-time: 66.21 s\n",
            "0.39 perplexity: 27.575 cost-time: 66.27 s\n",
            "0.48 perplexity: 26.785 cost-time: 66.49 s\n",
            "0.58 perplexity: 26.587 cost-time: 66.44 s\n",
            "0.68 perplexity: 26.464 cost-time: 66.54 s\n",
            "0.77 perplexity: 26.192 cost-time: 66.42 s\n",
            "0.87 perplexity: 26.127 cost-time: 66.54 s\n",
            "0.97 perplexity: 26.234 cost-time: 66.50 s\n",
            "Epoch: 20 Train Perplexity: 26.220\n",
            "model saving ...\n",
            "Done!\n",
            "Training Epoch: 21 ...\n",
            "0.10 perplexity: 25.988 cost-time: 74.29 s\n",
            "0.19 perplexity: 26.592 cost-time: 66.30 s\n",
            "0.29 perplexity: 26.425 cost-time: 66.36 s\n",
            "0.39 perplexity: 24.900 cost-time: 66.39 s\n",
            "0.48 perplexity: 24.069 cost-time: 66.51 s\n",
            "0.58 perplexity: 23.763 cost-time: 66.32 s\n",
            "0.68 perplexity: 23.845 cost-time: 66.30 s\n",
            "0.77 perplexity: 23.697 cost-time: 66.88 s\n",
            "0.87 perplexity: 23.574 cost-time: 66.81 s\n",
            "0.97 perplexity: 23.574 cost-time: 66.58 s\n",
            "Epoch: 21 Train Perplexity: 23.536\n",
            "Training Epoch: 22 ...\n",
            "0.10 perplexity: 22.195 cost-time: 74.14 s\n",
            "0.19 perplexity: 23.500 cost-time: 66.59 s\n",
            "0.29 perplexity: 23.207 cost-time: 66.37 s\n",
            "0.39 perplexity: 22.104 cost-time: 66.31 s\n",
            "0.48 perplexity: 21.436 cost-time: 66.28 s\n",
            "0.58 perplexity: 21.115 cost-time: 66.30 s\n",
            "0.68 perplexity: 21.106 cost-time: 66.40 s\n",
            "0.77 perplexity: 21.031 cost-time: 66.55 s\n",
            "0.87 perplexity: 21.093 cost-time: 66.76 s\n",
            "0.97 perplexity: 21.265 cost-time: 66.72 s\n",
            "Epoch: 22 Train Perplexity: 21.215\n",
            "Training Epoch: 23 ...\n",
            "0.10 perplexity: 19.947 cost-time: 74.06 s\n",
            "0.19 perplexity: 22.364 cost-time: 66.72 s\n",
            "0.29 perplexity: 21.803 cost-time: 66.71 s\n",
            "0.39 perplexity: 20.822 cost-time: 66.46 s\n",
            "0.48 perplexity: 19.930 cost-time: 66.70 s\n",
            "0.58 perplexity: 19.438 cost-time: 66.67 s\n",
            "0.68 perplexity: 19.253 cost-time: 66.64 s\n",
            "0.77 perplexity: 18.990 cost-time: 66.42 s\n",
            "0.87 perplexity: 18.900 cost-time: 66.39 s\n",
            "0.97 perplexity: 19.052 cost-time: 66.43 s\n",
            "Epoch: 23 Train Perplexity: 18.989\n",
            "Training Epoch: 24 ...\n",
            "0.10 perplexity: 17.403 cost-time: 73.86 s\n",
            "0.19 perplexity: 18.927 cost-time: 66.38 s\n",
            "0.29 perplexity: 18.826 cost-time: 66.43 s\n",
            "0.39 perplexity: 17.988 cost-time: 66.38 s\n",
            "0.48 perplexity: 17.480 cost-time: 66.36 s\n",
            "0.58 perplexity: 17.099 cost-time: 66.35 s\n",
            "0.68 perplexity: 17.009 cost-time: 66.31 s\n",
            "0.77 perplexity: 16.789 cost-time: 66.37 s\n",
            "0.87 perplexity: 16.707 cost-time: 66.35 s\n",
            "0.97 perplexity: 16.891 cost-time: 66.36 s\n",
            "Epoch: 24 Train Perplexity: 16.866\n",
            "Training Epoch: 25 ...\n",
            "0.10 perplexity: 15.398 cost-time: 73.92 s\n",
            "0.19 perplexity: 16.825 cost-time: 66.45 s\n",
            "0.29 perplexity: 16.908 cost-time: 66.53 s\n",
            "0.39 perplexity: 16.166 cost-time: 66.41 s\n",
            "0.48 perplexity: 15.761 cost-time: 66.57 s\n",
            "0.58 perplexity: 15.413 cost-time: 66.37 s\n",
            "0.68 perplexity: 15.304 cost-time: 66.33 s\n",
            "0.77 perplexity: 15.248 cost-time: 66.38 s\n",
            "0.87 perplexity: 15.249 cost-time: 66.57 s\n",
            "0.97 perplexity: 15.427 cost-time: 66.99 s\n",
            "Epoch: 25 Train Perplexity: 15.405\n",
            "model saving ...\n",
            "Done!\n",
            "Training Epoch: 26 ...\n",
            "0.10 perplexity: 13.746 cost-time: 75.26 s\n",
            "0.19 perplexity: 14.976 cost-time: 67.32 s\n",
            "0.29 perplexity: 15.144 cost-time: 67.20 s\n",
            "0.39 perplexity: 14.579 cost-time: 67.24 s\n",
            "0.48 perplexity: 14.371 cost-time: 67.13 s\n",
            "0.58 perplexity: 14.143 cost-time: 67.20 s\n",
            "0.68 perplexity: 13.995 cost-time: 67.52 s\n",
            "0.77 perplexity: 14.005 cost-time: 67.53 s\n",
            "0.87 perplexity: 13.948 cost-time: 67.66 s\n",
            "0.97 perplexity: 14.088 cost-time: 67.45 s\n",
            "Epoch: 26 Train Perplexity: 14.074\n",
            "Training Epoch: 27 ...\n",
            "0.10 perplexity: 14.049 cost-time: 74.94 s\n",
            "0.19 perplexity: 14.737 cost-time: 67.27 s\n",
            "0.29 perplexity: 14.992 cost-time: 67.32 s\n",
            "0.39 perplexity: 14.415 cost-time: 67.35 s\n",
            "0.48 perplexity: 13.954 cost-time: 67.45 s\n",
            "0.58 perplexity: 13.631 cost-time: 67.27 s\n",
            "0.68 perplexity: 13.431 cost-time: 67.25 s\n",
            "0.77 perplexity: 13.270 cost-time: 67.45 s\n",
            "0.87 perplexity: 13.208 cost-time: 67.29 s\n",
            "0.97 perplexity: 13.269 cost-time: 67.37 s\n",
            "Epoch: 27 Train Perplexity: 13.245\n",
            "Training Epoch: 28 ...\n",
            "0.10 perplexity: 13.613 cost-time: 74.86 s\n",
            "0.19 perplexity: 13.460 cost-time: 67.40 s\n",
            "0.29 perplexity: 13.790 cost-time: 67.47 s\n",
            "0.39 perplexity: 13.534 cost-time: 67.54 s\n",
            "0.48 perplexity: 13.076 cost-time: 67.44 s\n",
            "0.58 perplexity: 12.831 cost-time: 67.54 s\n",
            "0.68 perplexity: 12.637 cost-time: 67.51 s\n",
            "0.77 perplexity: 12.438 cost-time: 67.44 s\n",
            "0.87 perplexity: 12.409 cost-time: 67.52 s\n",
            "0.97 perplexity: 12.450 cost-time: 67.45 s\n",
            "Epoch: 28 Train Perplexity: 12.420\n",
            "Training Epoch: 29 ...\n",
            "0.10 perplexity: 12.274 cost-time: 75.02 s\n",
            "0.19 perplexity: 12.374 cost-time: 67.41 s\n",
            "0.29 perplexity: 12.295 cost-time: 67.44 s\n",
            "0.39 perplexity: 12.007 cost-time: 67.48 s\n",
            "0.48 perplexity: 11.767 cost-time: 67.48 s\n",
            "0.58 perplexity: 11.480 cost-time: 67.52 s\n",
            "0.68 perplexity: 11.319 cost-time: 67.47 s\n",
            "0.77 perplexity: 11.158 cost-time: 67.44 s\n",
            "0.87 perplexity: 11.157 cost-time: 67.38 s\n",
            "0.97 perplexity: 11.197 cost-time: 67.46 s\n",
            "Epoch: 29 Train Perplexity: 11.158\n",
            "Training Epoch: 30 ...\n",
            "0.10 perplexity: 10.967 cost-time: 74.65 s\n",
            "0.19 perplexity: 11.433 cost-time: 67.26 s\n",
            "0.29 perplexity: 11.323 cost-time: 67.19 s\n",
            "0.39 perplexity: 10.909 cost-time: 67.24 s\n",
            "0.48 perplexity: 10.879 cost-time: 67.27 s\n",
            "0.58 perplexity: 10.654 cost-time: 67.30 s\n",
            "0.68 perplexity: 10.560 cost-time: 67.36 s\n",
            "0.77 perplexity: 10.430 cost-time: 67.29 s\n",
            "0.87 perplexity: 10.382 cost-time: 67.33 s\n",
            "0.97 perplexity: 10.412 cost-time: 67.52 s\n",
            "Epoch: 30 Train Perplexity: 10.385\n",
            "model saving ...\n",
            "Done!\n",
            "Training Epoch: 31 ...\n",
            "0.10 perplexity: 9.842 cost-time: 75.59 s\n",
            "0.19 perplexity: 10.367 cost-time: 67.57 s\n",
            "0.29 perplexity: 10.390 cost-time: 67.64 s\n",
            "0.39 perplexity: 9.890 cost-time: 67.50 s\n",
            "0.48 perplexity: 9.784 cost-time: 67.55 s\n",
            "0.58 perplexity: 9.765 cost-time: 67.42 s\n",
            "0.68 perplexity: 9.697 cost-time: 67.49 s\n",
            "0.77 perplexity: 9.580 cost-time: 67.70 s\n",
            "0.87 perplexity: 9.571 cost-time: 67.52 s\n",
            "0.97 perplexity: 9.598 cost-time: 67.46 s\n",
            "Epoch: 31 Train Perplexity: 9.573\n",
            "Training Epoch: 32 ...\n",
            "0.10 perplexity: 8.789 cost-time: 74.98 s\n",
            "0.19 perplexity: 9.222 cost-time: 67.52 s\n",
            "0.29 perplexity: 9.201 cost-time: 67.46 s\n",
            "0.39 perplexity: 8.868 cost-time: 67.66 s\n",
            "0.48 perplexity: 8.674 cost-time: 67.55 s\n",
            "0.58 perplexity: 8.696 cost-time: 67.67 s\n",
            "0.68 perplexity: 8.655 cost-time: 68.10 s\n",
            "0.77 perplexity: 8.600 cost-time: 67.69 s\n",
            "0.87 perplexity: 8.622 cost-time: 67.84 s\n",
            "0.97 perplexity: 8.673 cost-time: 67.46 s\n",
            "Epoch: 32 Train Perplexity: 8.660\n",
            "Training Epoch: 33 ...\n",
            "0.10 perplexity: 7.685 cost-time: 74.97 s\n",
            "0.19 perplexity: 8.254 cost-time: 67.71 s\n",
            "0.29 perplexity: 8.314 cost-time: 67.56 s\n",
            "0.39 perplexity: 8.015 cost-time: 67.49 s\n",
            "0.48 perplexity: 7.898 cost-time: 67.46 s\n",
            "0.58 perplexity: 7.909 cost-time: 67.50 s\n",
            "0.68 perplexity: 7.898 cost-time: 67.49 s\n",
            "0.77 perplexity: 7.866 cost-time: 67.41 s\n",
            "0.87 perplexity: 7.891 cost-time: 67.45 s\n",
            "0.97 perplexity: 7.928 cost-time: 67.56 s\n",
            "Epoch: 33 Train Perplexity: 7.912\n",
            "Training Epoch: 34 ...\n",
            "0.10 perplexity: 7.288 cost-time: 75.04 s\n",
            "0.19 perplexity: 7.677 cost-time: 67.42 s\n",
            "0.29 perplexity: 7.794 cost-time: 67.25 s\n",
            "0.39 perplexity: 7.640 cost-time: 67.78 s\n",
            "0.48 perplexity: 7.500 cost-time: 67.60 s\n",
            "0.58 perplexity: 7.525 cost-time: 67.35 s\n",
            "0.68 perplexity: 7.539 cost-time: 67.46 s\n",
            "0.77 perplexity: 7.477 cost-time: 67.37 s\n",
            "0.87 perplexity: 7.506 cost-time: 67.46 s\n",
            "0.97 perplexity: 7.551 cost-time: 67.36 s\n",
            "Epoch: 34 Train Perplexity: 7.526\n",
            "Training Epoch: 35 ...\n",
            "0.10 perplexity: 6.554 cost-time: 74.95 s\n",
            "0.19 perplexity: 7.019 cost-time: 67.48 s\n",
            "0.29 perplexity: 7.096 cost-time: 67.52 s\n",
            "0.39 perplexity: 6.916 cost-time: 67.64 s\n",
            "0.48 perplexity: 6.816 cost-time: 67.51 s\n",
            "0.58 perplexity: 6.745 cost-time: 67.46 s\n",
            "0.68 perplexity: 6.782 cost-time: 67.45 s\n",
            "0.77 perplexity: 6.741 cost-time: 67.68 s\n",
            "0.87 perplexity: 6.774 cost-time: 67.67 s\n",
            "0.97 perplexity: 6.824 cost-time: 67.60 s\n",
            "Epoch: 35 Train Perplexity: 6.811\n",
            "model saving ...\n",
            "Done!\n",
            "Training Epoch: 36 ...\n",
            "0.10 perplexity: 6.343 cost-time: 75.73 s\n",
            "0.19 perplexity: 6.700 cost-time: 67.86 s\n",
            "0.29 perplexity: 6.766 cost-time: 67.85 s\n",
            "0.39 perplexity: 6.567 cost-time: 67.94 s\n",
            "0.48 perplexity: 6.451 cost-time: 67.90 s\n",
            "0.58 perplexity: 6.363 cost-time: 67.83 s\n",
            "0.68 perplexity: 6.340 cost-time: 67.94 s\n",
            "0.77 perplexity: 6.338 cost-time: 67.83 s\n",
            "0.87 perplexity: 6.399 cost-time: 67.85 s\n",
            "0.97 perplexity: 6.440 cost-time: 67.76 s\n",
            "Epoch: 36 Train Perplexity: 6.425\n",
            "Training Epoch: 37 ...\n",
            "0.10 perplexity: 5.566 cost-time: 75.42 s\n",
            "0.19 perplexity: 5.995 cost-time: 67.76 s\n",
            "0.29 perplexity: 6.094 cost-time: 67.73 s\n",
            "0.39 perplexity: 5.960 cost-time: 67.61 s\n",
            "0.48 perplexity: 5.882 cost-time: 67.73 s\n",
            "0.58 perplexity: 5.827 cost-time: 67.76 s\n",
            "0.68 perplexity: 5.774 cost-time: 67.52 s\n",
            "0.77 perplexity: 5.777 cost-time: 67.66 s\n",
            "0.87 perplexity: 5.836 cost-time: 67.65 s\n",
            "0.97 perplexity: 5.889 cost-time: 67.77 s\n",
            "Epoch: 37 Train Perplexity: 5.878\n",
            "Training Epoch: 38 ...\n",
            "0.10 perplexity: 5.198 cost-time: 75.25 s\n",
            "0.19 perplexity: 5.555 cost-time: 67.63 s\n",
            "0.29 perplexity: 5.661 cost-time: 67.56 s\n",
            "0.39 perplexity: 5.537 cost-time: 67.64 s\n",
            "0.48 perplexity: 5.451 cost-time: 67.59 s\n",
            "0.58 perplexity: 5.417 cost-time: 67.67 s\n",
            "0.68 perplexity: 5.405 cost-time: 67.45 s\n",
            "0.77 perplexity: 5.409 cost-time: 67.55 s\n",
            "0.87 perplexity: 5.461 cost-time: 67.43 s\n",
            "0.97 perplexity: 5.513 cost-time: 67.33 s\n",
            "Epoch: 38 Train Perplexity: 5.507\n",
            "Training Epoch: 39 ...\n",
            "0.10 perplexity: 4.966 cost-time: 75.01 s\n",
            "0.19 perplexity: 5.369 cost-time: 67.30 s\n",
            "0.29 perplexity: 5.484 cost-time: 67.34 s\n",
            "0.39 perplexity: 5.326 cost-time: 67.18 s\n",
            "0.48 perplexity: 5.254 cost-time: 67.30 s\n",
            "0.58 perplexity: 5.212 cost-time: 67.21 s\n",
            "0.68 perplexity: 5.172 cost-time: 67.25 s\n",
            "0.77 perplexity: 5.146 cost-time: 67.45 s\n",
            "0.87 perplexity: 5.203 cost-time: 67.16 s\n",
            "0.97 perplexity: 5.262 cost-time: 67.19 s\n",
            "Epoch: 39 Train Perplexity: 5.257\n",
            "Training Epoch: 40 ...\n",
            "0.10 perplexity: 4.704 cost-time: 74.60 s\n",
            "0.19 perplexity: 5.013 cost-time: 67.35 s\n",
            "0.29 perplexity: 5.103 cost-time: 67.16 s\n",
            "0.39 perplexity: 4.970 cost-time: 67.18 s\n",
            "0.48 perplexity: 4.965 cost-time: 67.21 s\n",
            "0.58 perplexity: 4.931 cost-time: 67.11 s\n",
            "0.68 perplexity: 4.886 cost-time: 67.32 s\n",
            "0.77 perplexity: 4.830 cost-time: 67.08 s\n",
            "0.87 perplexity: 4.883 cost-time: 67.26 s\n",
            "0.97 perplexity: 4.917 cost-time: 67.14 s\n",
            "Epoch: 40 Train Perplexity: 4.905\n",
            "model saving ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jKiydJsL94c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "21655340-e8ff-4293-f3f6-092b1c100548"
      },
      "source": [
        "!python generate.py \"漂向北方 別問我家鄉\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "2019-12-30 01:16:08.998518: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f54d6fc8780>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
            "model loading ...\n",
            "Done!\n",
            "Generated Result: ['漂', '向', '北', '方', ' ', '別', '問', '我', '家', '鄉', '牽', '味', '的', '魂', '憤', '\\n', '\\n', '你', '是', '多', '麼', '的', '感', '覺', '\\n', '\\n', '我', '不', '能', '說', ' ', '愛', '這', '種', '事', '\\n', '雖', '然', '美', '麗', '的', '輪', '軸', '\\n', '這', '一', '條', '大', '街', '\\u3000', '我', '的', '成', '唇', '\\u3000', '在', '我', '的', '溫', '柔', '鄉', '\\n', '神', '樂', '你', '的', '月', '容', '\\u3000', '是', '我', '的', '心', '容', '\\n', '＾', '\\n', '\\n', '\\n', '你', '是', '多', '情', '\\u3000', '當', '我', '走', '得', '好', '\\n', '卻', '像', '你', '的', '人', ' ', '是', '我', '的', ' ', 'B', 'A', 'D', ' ', 'B', 'O', 'Y', ' ', '難', '道', '我', '曾', '經', '默', '默', '縱', '容', '\\u3000', '那', '全', '愛', '情', '犯', '的', '酒', '\\n', '喔', '\\n', '\\n', 'B', 'A', 'D', ' ', 'B', 'O', 'Y', '\\u3000', 'B', 'A', 'D', ' ', 'B', 'O', 'Y', ' ', '我', '的', '壞', '讓', '我', '太', '無', '奈', '\\n', 'B', 'A', 'D', ' ', 'B', 'O', 'Y', '\\u3000', '我', '的', '壞', '讓', '我', '難', '過', '\\n', '這', '一', '刻', '\\u3000', '右', '一', '個', '人', '\\n', '是', '我', '的', '心', '容', '\\u3000', '需', '有', '煙', '容', '的', '黎', '覺', '\\n', '\\n', '我', '的', '淚', '沒', '有', '流', '的', '我', '\\u3000', '一', '輩', '鐘', ' ', '你', '仍', '然']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqTKof0kYR0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}